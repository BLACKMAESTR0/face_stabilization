## FACE STABILIZATION
<img width="1899" height="473" alt="Без имени" src="https://github.com/user-attachments/assets/92c8fbe6-87b8-486a-ad1c-c826edd76eb7" />






## Работа с Python
Версия Python: 3.11.5
1. Создать локальное окружение: `python -m venv .venv`
2. Установить все зависимости: `pip install -r requirements.txt`


## Быстрый старт
1. Запустить проект: 
`python main.py`


## Описание
Проект "face_stabilization" представляет собой пример реализации задачи детекции лица в реальном времени с дополнительной адаптацией к будущим позициям лица. 
Для запуска проекта необходима подключенная к устройству камера.


## Модели
В качестве основной модели детекции лица использовалась Yolo "face_yolov8n.pt"
https://huggingface.co/Bingsu/adetailer/blob/main/face_yolov8n.pt

Для прогнозирования будущих позиций лица на видео была обучена LSTM модель, принимающая n последовательных bbox'ов, указывающих на положение лица на кадре, и выдающая k предполагаемых bbox'ов для следующих k кадров.

На практике LSTM используется как модель, оценивающая долгосрочную динамику движения, поэтому используется последнее предсказание из горизонта, как наиболее информативное о направлении и инерции движения.

Процесс обучения указан в блокноте models/lstm_train.ipynb

При обучении использовался открытый датасет "YouTube Faces"
https://www.cs.tau.ac.il/~wolf/ytfaces/


## Работа с параметрами
При работе проекта возможно динамическое изменение параметров.


1. ALPHA_YOLO - влияние модели YOLO на каждый bbox (от 0 до 1) (Q↓ / W↑)
2. ALPHA_LSTM - влияение обученной модели LSTM на каждый bbox (от 0 до 1) (A↓ / S↑)
3. SEQ_LEN - количество последних кадров, подаваемых модели LSTM для предсказаний. (от 1 до 30) (Z↓ / X↑)
4. PRED_LEN - номер кадра, который берется из набора кадров, прогнозируемых LSTM. Bbox из этого кадра влияет на текущий bbox. (от 1 до 30) (C↓ / V↑)


Дополнительное пояснение PRED_LEN: Если PRED_LEN = 25, то на текущем кадре мы берем предсказанный LSTM bbox для 25 кадра в качестве решения по текущему кадру.

